# Hyperparameter Optimization Configuration
# AutoML and hyperparameter tuning settings

# Study Configuration
study:
  name: "model_optimization"
  direction: "maximize"  # maximize, minimize
  metric: "val_f1"
  sampler: "tpe"  # tpe, random, grid, cmaes
  pruner: "hyperband"  # hyperband, median, successive_halving

# Search Space Definition
search_space:
  # Model Selection
  model_type:
    type: "categorical"
    choices: ["random_forest", "xgboost", "lightgbm", "neural_network"]

  # Random Forest
  random_forest:
    n_estimators:
      type: "int"
      low: 50
      high: 500
      step: 50
    max_depth:
      type: "int"
      low: 3
      high: 20
    min_samples_split:
      type: "int"
      low: 2
      high: 20
    min_samples_leaf:
      type: "int"
      low: 1
      high: 10

  # XGBoost
  xgboost:
    n_estimators:
      type: "int"
      low: 100
      high: 1000
    max_depth:
      type: "int"
      low: 3
      high: 12
    learning_rate:
      type: "float"
      low: 0.01
      high: 0.3
      log: true
    subsample:
      type: "float"
      low: 0.6
      high: 1.0
    colsample_bytree:
      type: "float"
      low: 0.6
      high: 1.0
    reg_alpha:
      type: "float"
      low: 1e-8
      high: 10.0
      log: true
    reg_lambda:
      type: "float"
      low: 1e-8
      high: 10.0
      log: true

  # LightGBM
  lightgbm:
    n_estimators:
      type: "int"
      low: 100
      high: 1000
    num_leaves:
      type: "int"
      low: 20
      high: 150
    max_depth:
      type: "int"
      low: 3
      high: 12
    learning_rate:
      type: "float"
      low: 0.01
      high: 0.3
      log: true
    feature_fraction:
      type: "float"
      low: 0.6
      high: 1.0
    bagging_fraction:
      type: "float"
      low: 0.6
      high: 1.0
    min_child_samples:
      type: "int"
      low: 5
      high: 100

  # Neural Network
  neural_network:
    hidden_layers:
      type: "int"
      low: 1
      high: 5
    hidden_units:
      type: "categorical"
      choices: [32, 64, 128, 256, 512]
    dropout_rate:
      type: "float"
      low: 0.0
      high: 0.5
    learning_rate:
      type: "float"
      low: 1e-5
      high: 1e-2
      log: true
    batch_size:
      type: "categorical"
      choices: [16, 32, 64, 128]
    optimizer:
      type: "categorical"
      choices: ["adam", "sgd", "rmsprop"]
    activation:
      type: "categorical"
      choices: ["relu", "gelu", "selu"]

# Optimization Settings
optimization:
  n_trials: 100
  timeout_seconds: 3600
  n_jobs: 4
  cv_folds: 5

  # Early stopping
  early_stopping:
    enabled: true
    patience: 20
    min_trials: 10

  # Parallelization
  parallel:
    enabled: true
    n_workers: 4
    storage: "sqlite:///optuna_study.db"

# Model Compression
compression:
  quantization:
    enabled: true
    method: "dynamic"  # dynamic, static, qat
    dtype: "int8"

  pruning:
    enabled: true
    method: "magnitude"  # magnitude, structured, unstructured
    amount: 0.3

  distillation:
    enabled: false
    teacher_model: "large_model"
    temperature: 3.0
    alpha: 0.5

# Export Configuration
export:
  formats:
    - "pickle"
    - "onnx"
    - "joblib"

  save_trials: true
  save_best_model: true
  output_dir: "outputs/optimization"
